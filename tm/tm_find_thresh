#!/bin/env python

import argparse
import os
import numpy as np
import mrcfile
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit


ext = ".png" # Extension for saving the plots

parser = argparse.ArgumentParser(description="""
                                 
Calculate threshold for picking particles based on distribution of scores in a template matching correlation map. In principle, it does not matter where the score map comes from (pytom-match-pick, STOPGAP, Dynamo, etc).
The assumption is that the true positives lie on the (approximately) linear part of the plot of sorted scores.
                                 
Optionally, a mask can be provided to focus the calculation only in the regions of interest.
                                 
This approach is an attempt to automate the manual threshold definition method implemented in Dynamo and STOPGAP.
                                 
""",
formatter_class=argparse.ArgumentDefaultsHelpFormatter
)


parser.add_argument("--scoremap", 
                    type=str, 
                    required=True, 
                    help="Path to the input scoremap."
                    )
parser.add_argument("--mask", 
                    type=str, 
                    required=False, 
                    help="Path to a tomogram mask, which must have same dimensions as the input score map. Could be a slab mask created by Slabify, for example."
                    )
parser.add_argument("--minval", 
                    type=float,
                    default=0.0, 
                    required=False, 
                    help="Minimum score to consider in the analysis. Only values greater than this will be considered."
                    )
parser.add_argument("--ini", 
                    type=int,
                    default=1000, 
                    required=False, 
                    help="Initial number of points to start fitting."
                    )
parser.add_argument("--mult", 
                    type=float,
                    default=1.1, 
                    required=False, 
                    help="Multiplier for growing the number of points used in the iterative fitting. Should be greater than 1. Higher values converge faster but give less accurate results."
                    )
parser.add_argument("--tol", 
                    type=float,
                    default=0.01, 
                    required=False, 
                    help="The maximum tolerated Mean Absolute Error. This value decides when the plot ceases to be linear. Should be greater than 0. Lower values are more stringent."
                    )
parser.add_argument("--save_thr_scoremap", 
                    default=False, 
                    action="store_true",
                    required=False, 
                    help="Save thresholded (and possibly masked) scoremap."
                    )
parser.add_argument("--verbose", 
                    default=False, 
                    action="store_true",
                    required=False, 
                    help="Print detailed information about the fitting at every iteration."
                    )

args = parser.parse_args()

# Print usage information
if not args.scoremap:
    parser.print_help()

# Get rootname of scoremap:
stfilename = os.path.basename(args.scoremap)
strootname, _ = os.path.splitext(stfilename)

inmrc = mrcfile.open(args.scoremap, permissive=True)
scores = np.array(inmrc.data)
apix = inmrc.voxel_size
del inmrc
if scores.dtype == 'float16':
    scores = scores.astype('float32')

if args.mask:

    inmask = mrcfile.open(args.mask, permissive=True)
    mask = np.array(inmask.data)
    del inmask
    # If mask is stored as bytes we need to convert it to binary (boolean):
    if mask.dtype == 'int8':
        mask = mask > -128
    if mask.dtype == 'float16':
        mask = mask.astype('float32')

    scores *= mask # Multiply the score map with the mask
    del mask

sorted = np.sort(scores[scores > args.minval])[::-1] # Sort all scores > 0 (or desired value) in descending order
if not args.save_thr_scoremap:
    del scores

n_points = args.ini
n_points_old = n_points
 
print(f'Total {len(sorted)} points')
iter=1
while True:
 
    y_data = sorted[:n_points]
    x_data = np.arange(n_points)
    # print(len(y_data),len(x_data))
 
    # Fit a linear model using polyfit
    slope, intercept = np.polyfit(x_data, y_data, deg=1)
    if args.verbose:
        print(f'Iteration {iter}, {n_points} points: {slope:.8f}x + {intercept:.8f}')
    y_pred = slope * x_data + intercept
 
    # Calculate residuals
    abs_residuals = np.abs(y_data - y_pred)
    # print(f'Absolute residuals: min: {abs_residuals.min():.6f}, max: {abs_residuals.max():.6f}, mean: {abs_residuals.mean():.6f}, median: {np.median(abs_residuals):.6f}, std: {abs_residuals.std():.6f}')

    slope_old = slope
    intercept_old = intercept
    n_points_old = n_points
    n_points = int(n_points * args.mult)
    # n_points += args.ini
    iter += 1
 
    if abs_residuals.mean() > args.tol:
        break
 
thr = sorted[n_points_old]
x_data = np.arange(n_points_old)
y_pred = slope_old * x_data + intercept_old
print(f'n_points = {n_points_old}, thr = {thr:.3f}')
 
# Plot the data and the fitted lines
plt.plot(sorted, label='Data points', color='c')
plt.plot(x_data, y_pred, label='Fitted line', color='m')
plt.xlabel('X')
plt.ylabel('Y')
plt.title(stfilename)
# Add a dashed horizontal line at y = y_value
plt.axhline(y=thr, color='k', linestyle='--', label=f'thr = {thr:.3f}')
# Ensure the tick for y_value is shown on the Y-axis
# plt.yticks(list(plt.yticks()[0]) + [thr])
plt.legend()
figname = strootname + "_sorted_reverse_linefit" + ext
plt.savefig(figname)
print("Plot saved as %s" % figname )
plt.close()

if args.save_thr_scoremap:
    scores[scores <= thr] = 0 # We make everything below the found threshold zero
    with mrcfile.new(strootname + "_thresholded.mrc", overwrite=True) as mrc:
        mrc.set_data(scores)
        mrc.voxel_size = apix
